import matplotlib.pyplot as plt
from featureExtractor import featureExtractor
from ann_visualizer.visualize import ann_viz
from keras.optimizers import Adam,SGD
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout,GlobalMaxPool1D
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import SpatialDropout1D,TimeDistributed
from keras.layers.recurrent import LSTM
import statistics

from sklearn.model_selection import train_test_split
import numpy as np

def evaluate_model(trainX, trainy, testX, testy, param):
	verbose, epochs, batch_size = 0, param["epochs"], param["batch_size"]
	n_timesteps, n_features, n_outputs = 7499,1,5
	model = Sequential()
	model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(n_timesteps,n_features)))
	model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))
	model.add(MaxPooling1D(pool_size=2))
	model.add(SpatialDropout1D(rate=0.01))

	model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	model.add(MaxPooling1D(pool_size=2))
	model.add(SpatialDropout1D(rate=0.01))

	model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	model.add(MaxPooling1D(pool_size=2))
	model.add(SpatialDropout1D(rate=0.01))

	# model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	# model.add(Conv1D(filters=32, kernel_size=param["kernel"], activation='relu',padding = "valid"))
	# model.add(MaxPooling1D(pool_size=2))
	# model.add(SpatialDropout1D(rate=0.01))

	model.add(Dropout(rate = 0.01))
	model.add(Dropout(rate = 0.05))
	# model.add(TimeDistributed(Conv1D(64, 32, activation='relu')))
	model.add(Flatten())
	# model.add(TimeDistributed(Conv1D(64, 32, activation='relu',input_shape = )))

	model.add(Dense(64, activation='relu'))
	# model.add(Dropout(rate = 0.5))
	model.add(Dense(n_outputs, activation='softmax'))
	model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01),metrics=['accuracy'])
	# print(model.summary())
	model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)
	loss, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)
	if(accuracy>0.35):
		print("successfull model saving")
		model.save("powerModel3")
	# print(loss)
	return accuracy


def run_experiment(params, repeats=10):
	# load data
	f = featureExtractor("hypnogram4.csv","eeg4.csv")
	data = f.getSignals()
	X = np.array(data[0])
	Y = np.array(data[1])
	print(X,Y)
	trainX, testX, trainy, testy = train_test_split(X, Y, test_size=0.33, random_state=42,shuffle = True)

	print(trainX.shape)
	print(trainy.shape)
	# test each parameter
	all_scores = list()
	for p in params:
		# repeat experiment
		scores = list()
		for r in range(repeats):
			score = evaluate_model(trainX, trainy, testX, testy, p)
			score = score * 100.0
			print('>p=%s #%d: %.3f' % (p, r+1, score))
			scores.append(score)
		print(p)
		print(statistics.mean(scores))
		print(statistics.stdev(scores))
		# all_scores.append(scores)
	# summarize results
	# summarize_results(all_scores, params)
 
# run the experiment
p1 = {
	"epochs":15,
	"batch_size":100,
	"kernel":2
}
p2 = {
	"epochs":20,
	"batch_size":100,
	"kernel":5
}
p3 = {
	"epochs":20,
	"batch_size":100,
	"kernel":11
}

n_params = [p1,p2,p3]
run_experiment(n_params)
# print(statistics.mean([1,3,4]))
# print(statistics.stdev([1,3,4]))
